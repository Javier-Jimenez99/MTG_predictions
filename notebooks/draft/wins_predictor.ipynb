{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from MTGpred.utils.mtgjson import simplify_name, parse_mana_cost\n",
    "import re\n",
    "import torch\n",
    "\n",
    "\n",
    "class AllPickedCardsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cards_df: pd.DataFrame,\n",
    "        picked_cards: pd.DataFrame,\n",
    "        model_name: str = \"xlnet-base-cased\",\n",
    "        cased: bool = True,\n",
    "        join_tokens: bool = True,\n",
    "        max_length: int = 4096,\n",
    "        truncation: bool = True,\n",
    "        include_name: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset for decks classification\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cards_df : pd.DataFrame\n",
    "            Dataframe with all the cards\n",
    "        picked_cards : pd.DataFrame\n",
    "            Dataframe with all the picked cards\n",
    "        model_name : str, optional\n",
    "            Name of the transformer model, by default \"xlnet-base-cased\"\n",
    "        cased : bool, optional\n",
    "            Cased the card output or not, by default True\n",
    "        join_tokens : bool, optional\n",
    "            Join all texts and tokenize all or not, by default False\n",
    "        max_length : int, optional\n",
    "            Max length of the tokenizer, by default 256\n",
    "        truncation : bool, optional\n",
    "            Truncate the text or not, by default True\n",
    "        \"\"\"\n",
    "\n",
    "        assert not join_tokens or (\n",
    "            join_tokens and truncation\n",
    "        ), \"If join_tokens is True, truncation must be True\"\n",
    "\n",
    "        self.cards_df = cards_df\n",
    "        self.data = picked_cards\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.cased = cased\n",
    "        self.join_tokens = join_tokens\n",
    "        self.max_length = max_length\n",
    "        self.truncation = truncation\n",
    "        self.include_name = include_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def preprocess_card(self, name: str):\n",
    "        simplified_name = simplify_name(name)\n",
    "        all_variations = []\n",
    "\n",
    "        selected_card = self.cards_df[\n",
    "            (self.cards_df[\"faceName\"] == simplified_name)\n",
    "            | (self.cards_df[\"name\"] == simplified_name)\n",
    "        ]\n",
    "        if len(selected_card) == 0:\n",
    "            print(\n",
    "                f\"WARNING: {name} cant be found in the database. Will be removed from the deck.\"\n",
    "            )\n",
    "            return []\n",
    "\n",
    "        for index, variations in selected_card.iterrows():\n",
    "            mana_cost = (\n",
    "                parse_mana_cost(variations[\"manaCost\"])\n",
    "                if not pd.isna(variations[\"manaCost\"])\n",
    "                else \"\"\n",
    "            )\n",
    "\n",
    "            card_type = variations[\"type\"]\n",
    "\n",
    "            text = variations[\"text\"] if not pd.isna(variations[\"text\"]) else \"\"\n",
    "            mana_in_text = re.findall(r\"\\{.*\\}\", text)\n",
    "            for mana in mana_in_text:\n",
    "                text = text.replace(mana, parse_mana_cost(mana))\n",
    "\n",
    "            stats = f\"{variations['power']} power, {variations['power']} power\"\n",
    "\n",
    "            if self.include_name:\n",
    "                input_text = \". \".join([name, mana_cost, card_type, text, stats])\n",
    "            else:\n",
    "                input_text = \". \".join([mana_cost, card_type, text, stats])\n",
    "\n",
    "            if not self.cased:\n",
    "                input_text = input_text.lower()\n",
    "\n",
    "            all_variations.append(input_text)\n",
    "\n",
    "        return all_variations\n",
    "\n",
    "    def get_tokenized_text(self, picks_data):\n",
    "        all_cards = []\n",
    "\n",
    "        for card in picks_data:\n",
    "            all_cards.extend(self.preprocess_card(card))\n",
    "\n",
    "        if self.join_tokens:\n",
    "            all_cards = self.tokenizer.sep_token.join(all_cards)\n",
    "\n",
    "            tokenized_deck = self.tokenizer(\n",
    "                all_cards,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=self.truncation,\n",
    "            )\n",
    "        else:\n",
    "            tokenized_deck = []\n",
    "            for card in all_cards:\n",
    "                tokenized_deck.append(\n",
    "                    self.tokenizer(\n",
    "                        card,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=self.truncation,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return tokenized_deck\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        picks_data = self.data.iloc[idx]\n",
    "\n",
    "        picks = self.get_tokenized_text(picks_data[\"picks\"])\n",
    "\n",
    "        if self.join_tokens:\n",
    "            picks[\"labels\"] = torch.tensor(picks_data[\"wins\"])\n",
    "\n",
    "            return picks\n",
    "        else:\n",
    "            return {\n",
    "                \"input\": picks,\n",
    "                \"label\": picks_data[\"wins\"],\n",
    "                \"mask_length\": len(picks),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                 picks  wins\n",
       "0   [Trostani, Three Whispers, Shock, Dog Walker, ...     4\n",
       "0   [Commercial District, Outrageous Robbery, Tunn...     6\n",
       "0   [Leering Onlooker, Deadly Cover-Up, Harried Dr...     1\n",
       "0   [Surveillance Monitor, Cold Case Cracker, Ceas...     6\n",
       "0   [Fugitive Codebreaker, Galvanize, Shock, Mistw...     4\n",
       "..                                                ...   ...\n",
       "0   [Neighborhood Guardian, Seasoned Consultant, F...     1\n",
       "0   [Soul Enervation, Nightdrinker Moroii, Surveil...     2\n",
       "0   [Evidence Examiner, Axebane Ferox, Repulsive M...     1\n",
       "0   [Doppelgang, Hard Evidence, Tunnel Tipster, Vi...     7\n",
       "0   [No Witnesses, Wojek Investigator, Museum Nigh...     2\n",
       "\n",
       "[10738 rows x 2 columns]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_drafted_data(data_folder: str, num_drafts: int = 10000):\n",
    "    files = os.listdir(data_folder)\n",
    "    total_drafts = 0\n",
    "    drafts = []\n",
    "\n",
    "    for f in files:\n",
    "        df = pd.read_csv(os.path.join(data_folder, f))\n",
    "        groups = df.groupby(\"draft_id\")\n",
    "\n",
    "        for draft_id, group in groups:\n",
    "            drafts.append(\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"picks\": [list(group[\"pick\"])],\n",
    "                        \"wins\": group[\"event_match_wins\"].iloc[0],\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "        total_drafts += len(df[\"draft_id\"].unique())\n",
    "\n",
    "        if total_drafts >= num_drafts:\n",
    "            break\n",
    "\n",
    "    return pd.concat(drafts)\n",
    "\n",
    "\n",
    "df = get_drafted_data(\n",
    "    \"C:/Users/javij/OneDrive/Escritorio/Proyectos/MTG_predictions/data/draft/draft_data/\"\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2051\n",
       "2    1926\n",
       "0    1598\n",
       "3    1566\n",
       "4    1165\n",
       "7    1061\n",
       "5     806\n",
       "6     565\n",
       "Name: wins, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"wins\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from MTGpred.utils.mtgjson import load_cards_df\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cards_df = load_cards_df(data_path=\"../data/AtomicCards.json\")\n",
    "train_cards, test_cards = train_test_split(df, test_size=0.2)\n",
    "\n",
    "train_dataset = AllPickedCardsDataset(\n",
    "    cards_df, train_cards, model_name=\"allenai/longformer-base-4096\", cased=False\n",
    ")\n",
    "test_dataset = AllPickedCardsDataset(\n",
    "    cards_df, test_cards, model_name=\"allenai/longformer-base-4096\", cased=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"allenai/longformer-base-4096\", num_labels=3\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/longformer\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtg-preds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
